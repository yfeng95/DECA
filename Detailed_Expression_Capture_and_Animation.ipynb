{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOJqULOUbwW86YwxbpnWlFn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhoangvslev/DECA/blob/master/Detailed_Expression_Capture_and_Animation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "j0hyU8s5f_SB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bB5Ii6bkBYWc"
      },
      "outputs": [],
      "source": [
        "#@title Setup dependencies\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/mhoangvslev/DECA\n",
        "\n",
        "%cd DECA/\n",
        "!apt -q install -y zip unzip ffmpeg libsm6 libxext6\n",
        "#!pip install -r requirements.txt\n",
        "#!pip install 'torch==1.6.0'\n",
        "#!pip install 'torchvision==0.7.0'\n",
        "#!pip install -q 'pytorch3d==0.2.5'\n",
        "!pip install -q numpy scipy chumpy scikit-image opencv-python PyYAML face-alignment yacs kornia ninja fvcore gdown matplotlib\n",
        "!pip install -q gdown==4.5.4 --no-cache-dir\n",
        "#!pip install --upgrade ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install pytorch3d for Colab\n",
        "# SOURCE: https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md\n",
        "import sys\n",
        "import torch\n",
        "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "version_str=\"\".join([\n",
        "    f\"py3{sys.version_info.minor}_cu\",\n",
        "    torch.version.cuda.replace(\".\",\"\"),\n",
        "    f\"_pyt{pyt_version_str}\"\n",
        "])\n",
        "!pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yfHNyj6KpZ4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download models\n",
        "#@markdown By executing this cell, you agree to the [LICENSE](https://flame.is.tue.mpg.de/modellicense.html) provided by Max-Planck-Gesellschaft zur FÃ¶rderung der Wissenschaften e.V\n",
        "\n",
        "print(\"Downloading FLAME2020 model...\")\n",
        "!gdown 1FRoGBmNCLM6Q0FyP_IeqRtbwTsHslss5 -O FLAME2020.zip\n",
        "!unzip -o FLAME2020.zip -d data/\n",
        "\n",
        "print(\"Downloading deca_model...\")\n",
        "!gdown 1rp8kdyLPvErw2dTmqtjISRVvQLj6Yzje -O data/deca_model.tar"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZmSRqqrvCIwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face reconstruction"
      ],
      "metadata": {
        "id": "TuLFayzVf6h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -V"
      ],
      "metadata": {
        "id": "xm8YmZeopmci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep torch"
      ],
      "metadata": {
        "id": "BC_nEkL2i9cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run paper demo\n",
        "print(\"Setting up...\")\n",
        "!pip list | grep torch\n",
        "#!pip install -q kornia==0.4.0 yacs==0.1.8 face_alignment ninja fvcore\n",
        "\n",
        "print(\"Check for NVIDIA Driver...\")\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"Running experiments...\")\n",
        "import os\n",
        "input_folder = \"TestSamples/AFLW2000\" #@param {type:\"string\"}\n",
        "output_folder = os.path.join(input_folder, \"results\")\n",
        "!python demos/demo_reconstruct.py -i $input_folder -s $output_folder --saveDepth True --saveObj True\n"
      ],
      "metadata": {
        "id": "iiFP_JPZHjVf",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Use your own image\n",
        "#@markdown Upload your images to `upload` folder under `DECA`\n",
        "\n",
        "print(\"Check for NVIDIA Driver...\")\n",
        "!nvidia-smi\n",
        "\n",
        "print(\"Running experiments...\")\n",
        "import os\n",
        "input_folder = \"/content/\" #@param {type:\"string\"}\n",
        "output_folder = os.path.join(input_folder, \"results\")\n",
        "!python demos/demo_reconstruct.py -i $input_folder -s $output_folder --saveDepth True --saveObj True\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ny9lRkxefRPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize the results\n",
        "#@markdown Add import ctypes.util in glcontext.py if error. Warning: May crash your session!\n",
        "#@markdown Alternatively, you can use this [3dviewer.net](https://3dviewer.net/).\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Util function for loading meshes\n",
        "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_view_transform,\n",
        "    FoVPerspectiveCameras, \n",
        "    PointLights, \n",
        "    DirectionalLights, \n",
        "    Materials, \n",
        "    RasterizationSettings, \n",
        "    MeshRenderer, \n",
        "    MeshRasterizer,  \n",
        "    SoftPhongShader,\n",
        "    TexturesUV,\n",
        "    TexturesVertex\n",
        ")\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Initialize a camera.\n",
        "# With world coordinates +Y up, +X left and +Z in, the front of the cow is facing the -Z direction. \n",
        "# So we move the camera by 180 in the azimuth direction so it is facing the front of the cow. \n",
        "R, T = look_at_view_transform(2.7, 0, 180) \n",
        "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
        "\n",
        "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "# and blur_radius=0.0. We also set bin_size and max_faces_per_bin to None which ensure that \n",
        "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
        "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
        "# the difference between naive and coarse-to-fine rasterization. \n",
        "raster_settings = RasterizationSettings(\n",
        "    image_size=512, \n",
        "    blur_radius=0.0, \n",
        "    faces_per_pixel=1, \n",
        ")\n",
        "\n",
        "# Place a point light in front of the object. As mentioned above, the front of the cow is facing the \n",
        "# -z direction. \n",
        "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
        "\n",
        "# Create a Phong renderer by composing a rasterizer and a shader. The textured Phong shader will \n",
        "# interpolate the texture uv coordinates for each vertex, sample from a texture image and \n",
        "# apply the Phong lighting model\n",
        "renderer = MeshRenderer(\n",
        "    rasterizer=MeshRasterizer(\n",
        "        cameras=cameras, \n",
        "        raster_settings=raster_settings\n",
        "    ),\n",
        "    shader=SoftPhongShader(\n",
        "        device=device, \n",
        "        cameras=cameras,\n",
        "        lights=lights\n",
        "    )\n",
        ")\n",
        "\n",
        "results = next(os.walk(output_folder))[1]\n",
        "for folder in results:\n",
        "  filename = os.path.join(output_folder, folder)\n",
        "  mesh = load_objs_as_meshes([os.path.join(filename, f'{folder}.obj')], device=device)\n",
        "  #clr_map = load(os.path.join(filename, f'{folder}.png'))\n",
        "  images = renderer(mesh)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "  plt.axis(\"off\");\n",
        "  break\n"
      ],
      "metadata": {
        "id": "T_KrjopImyGQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuBCgeH08tdn",
        "cellView": "form"
      },
      "source": [
        "#@title Download the result\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(next(os.walk(output_folder)))\n",
        "folders = [ os.path.join(output_folder, f) for f in next(os.walk(os.path.join(input_folder, 'results')))[1] ]\n",
        "\n",
        "print(f'Download results...')\n",
        "os.system(f'zip -r download.zip {\" \".join(folders)}')\n",
        "files.download(\"download.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}